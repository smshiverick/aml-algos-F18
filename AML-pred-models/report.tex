\documentclass[sigconf]{acmart}

\input{format/final}

\begin{document}
  \title{Comparing Predictive Models of Pain Reliever Misuse and Abuse}
  \author{Sean M. Shiverick}
  \affiliation{
  \institution{Indiana University-Bloomington}
  }
\renewcommand{\shortauthors}{S.M. Shiverick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

The misuse and abuse of prescription opioids (MUPO) has become a major health 
crisis in the U.S. The rise of opioid overdose deaths in recent years is also 
correlated with the increased availability of illicit and synthetic opioids
\cite{nida18}. Predictive models can be used to identify demographic factors 
related to MUPO and may help predict individuals at risk for opioid addiction.
The present study compares several linear and non-linear classification models 
of pain reliever misuse and abuse to identify the models that best fit the data. 
The sample data consisted of N = 114,038 respondents from the National Survey 
on Drug Use and Health (NSDUH) from 2015 and 2016. Classification models 
vary in terms of complexity, accuracy, and interpretability. 

Simple models provide interpretable solutions but may have lower accuracy 
than more complex models which provide higher accuracy but are often more 
difficult to interpret.   

\footnote{ Address correspondence to \textit{smshiver@iu.edu}}
\end{abstract}
\keywords{Predictive Modeling, Supervised Learning, Classification Models}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The misuse and abuse of prescription opioids (MUPO) over the past two 
decades has become a major health crisis in the U.S. \cite{volkow14}. 
An estimated 2 million Americans suffered a substance use disorder related 
to prescription opioid pain relievers such as oxycodone or hydrocodone 
in 2015 \cite{nida18}. Opioid dependence and relapse are chronic health 
conditions and following treatment many addicted individuals are at high 
risk for relapse and overdose death \cite{shaham03}. From 1999 to 2016, 
the number of opioid-related overdose deaths has more than quadrupled. 
On average, more than 115 people die from an opioid overdose each day 
\cite{cdc18, judd16}. Supply-based interventions to reduce the availability 
of prescription opioids have produced a shift to the use of heroin and 
synthetic opioids such as fentanyl \cite{jones15}. The dosage levels and 
potency of illicit and synthetic opioids are largely unknown which has
increased the risk of overdose death. The sharp rise in prescription 
overdose deaths (POD) and heroin overdose deaths (HOD) are correlated 
\cite{muhuri13, unick13}. Predictive modeling approaches can be used to 
provide insights to inform policy decisions for addressing the opioid crisis. 
This study compares different classification models of pain reliever misuse 
and abuse to select the best predictive model(s) and identify important 
features that contribute to the misuse of prescription opioids. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the era of ``big data'', large amounts of health information are being 
generated from electronic medical records (EMRs), clinical research data, to 
population-level health data \cite{herland14}. It can be difficult to obtain 
reliable information about drug use based on self-reports; however, surveys 
provide data on a range of issues that people may be reluctant to disclose 
such as drug use and mental health problems. The current study considers
data from the National Survey on Drug Use and Health (NSDUH) which is a major 
source of information for the use of illicit drugs and mental health issues 
among the U.S. population aged 12 or older \cite{samhsa18}. The NSDUH is a 
comprehensive public survey thay includes more than 2600 variables on a 
diverse array of questions related to the use, misuse, and abuse of substances 
including alcohol, tobacco, prescription medications, and illicit drugs. 
In addition to typical demographic information, the survey includes self-
reported measures on items related to physical health, mental health (e.g., 
depression, anxiety, suicidal ideation), counseling, and drug and alcohol 
treatment. Data from the NSDUH has been used for identifying groups at high risk 
for substance use, and the co-occurrence of substance use and mental health 
disorders. The target variable for the study was any misuse or abuse of 
prescription pain relievers and the features of interest were selected 
demographic variables, medication usage, and use of illicit drugs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predictive Modeling}

Predictive modeling, statistical learning, or machine learning is a set 
of procedures and automated processes for extracting knowledge from data 
\cite{james13, kuhn13, muller17, raschka17}. The two main branches of 
predictive modeling are supervised learning and unsupervised learning. 
Supervised learning problems involve prediction about a specific target 
variable or outcome of interest . If a given dataset has no target outcome, 
unsupervised learning methods can be used to discover underlying structure 
in unlabeled data (e.g., clustering). Supervised learning is used to 
predict a certain outcome from a given input, when examples of input/output 
pairs are available in the data (e.g., logistic regression) \cite{muller17}. 
A statistical learning model is constructed on a set of observation used to
train the model set and can then be used to predict new observations. Two major 
approaches to supervised learning problems are regression and classification. 
When the target variable to be predicted is continuous, or there is continuity 
between the outcome (e.g., home values, income), a regression model is used 
to test the set of features that predict the target variable. If the target is 
a class label, set of categorical or binary outcomes (e.g., spam or ham emails, 
benign or malignant cells), then classification is used to predict which class 
or category label that new instances will be assigned to. The present study 
uses a supervised learning approach to classify instances of pain reliever 
misuse and abuse using several different classification models. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Metrics for Evaluating Prediction Models}

Evaluating the performance of learning algorithms can be helpful for 
selecting the best model for a given problem. Binary classification is 
assessed in terms of the successful assignment of observations to one of two 
classes: positive or negative. Medical testing outcomes are often used 
as an example to illustrate classification decisions and errors. A person 
is either diagnosed with an illness or not, and the person actually has the 
illness or not. In the present case, individuals are classified either as 
having misused and abused pain relievers or not, and the predictions are either 
correct or incorrect. Model performance is evaluated using accuracy which is 
defined by the number of correct predictions divided by the total number of 
all samples. Any model selected cannot make perfect predictions as there are 
always mistakes to be found. For example, a negative instance can be labeled 
as positive: a person who has never misused or abused pain relievers may be 
classified as having done so (i.e., 'false positive'). Conversely, a positive 
instance may be classified as negative: a person who has misused and abused 
pain relievers may be labeled as never having done so (i.e., 'false negative').

Classification errors and correct decisions are represented in a 
\emph{confusion matrix} (Table 1) that indicates the correspondence between 
predicted and actual outcomes. The confusion matrix is a two-by-two array in 
which the columns correspond to the actual observed classes and the rows 
correspond to the predicted classes. The main diagonal indicate the number of 
correctly classified samples (\emph{true positive, true negative}, while the 
other entries represent the number of samples in one class that were mistakenly 
classified as another class. Classification models are evaluated using several 
measures including recall, precision, and the $f_1$-score \cite{wiki18}. 
\emph{Recall} or ``sensitivity'' measures how many positive samples are 
captured by the positive predictions ( \(\frac{TP}{TP+FN}\) ), and is used 
when we want to identify all positive samples while avoiding false negatives. 
\emph{Precision} or the ``positive predictive value'', measures how many of 
the samples predicted as positive are actually positive 
( \(\frac{TP}{TP+FP}\) ), and is used as a metric when the goal is to limit 
the number of false positives. The \emph{$f_1$-score} or `f-measure' provides 
the harmonic mean of precision and recall. The $f_1$-score can be a better 
metric than accuracy in datasets with \emph{imbalanced classes}, where one 
class is much more frequent than the other class, as it takes recall and 
precision into account \cite{muller17}.

\begin{equation}
  \ f_1 score = 2*\frac{Precision*Recall}{Precision+Recall}\
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Confusion Matrix for Evaluating Classification Model Performance}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
     &  &  Actual Outcome & \\
    \midrule
     Predicted & Outcome & No Misuse & PRL Misuse \\
    \midrule
     & No Misuse & \textbf{True Negative} & False Positive \\
    \midrule
     & PRL Misuse & False Negative & \textbf{True Positive} \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training and Test Set Accuracy}

An important goal in constructing and evaluating predictive models is to 
select a model that performs well not only with data used to train the model, 
but also with new observations that are previously `unseen'. A standard 
practice is to divide the sample dataset into two portions: a 
\emph{training set} and a \emph{testing set} of observations that is set 
aside and used to evaluate model performance. By convention, approximately 
70 to 80 percent of observations are used in the training set and the 
remaining observations are held in the testing set. Two main problems can 
occur in evaluating model performance: overfitting and underfitting. 
In the case of \emph{overfitting}, a model can have high accuracy on the 
training set but perform poorly with new data in the test set because the 
model is `over-fit' to the training data, By contrast, ``underfitting'' 
occurs when a model performs poorly with the training data but has high 
accuracy with the new observations in the testing set. One of the simplest 
predictive models, K-Nearest neighbors, classifies observations by 
assigning the label that is most frequent among the `k' number of nearest
training samples that is selected by the user. The accuracy of the KNN
classifier for the training set and testing set is plotted as a function 
of the parameter k-neighbors in Figure 1. The plot shows a tradeoff in 
training accuracy versus testing accuracy; higher accuracy on the testing 
set is associated with a slight decrease in performance on the training set. 
The best model is one that optimizes testing set accuracy, while striking 
a balance between the problems of overfitting and underfitting. In this case,
performance on the test set increased slightly between 2 and 4 neighbors, 
but did not improve much beyond 5 neighbors. Therefore, selecting k=4 
neighbors was selected would provide a good solution for the model fit. 

 \begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure1.pdf}
  \caption{K-Nearest Neighbors Classifier Accuracy for Training Set and 
  Testing Set as a function of Number of Neighbors}
  \label{f:Figure1}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classification Models}

\subsubsection{Linear Classifier Models}

As stated by the statistician George Box, "All models are wrong, but some 
models are useful." \cite{box05}. There are advantages and limitations for
selecting any classification model; however, ogistic regression is one of 
the most useful, powerful, and interpretable models available. Logistic 
regression models the probability that a target outcome \emph{$Y_i$} 
belongs to a particular category or binary outcome by calculating the 
relative weight of each feature for predicting the probabilities of a 
certain outcome \cite{raschka17}. The decision boundary for the logistic 
regression classifier is a linear function of the input; a binary classifier 
separates two classes using along a line, plane, or hyperplane 
\cite{muller17}. Given that the probability values for the outcome 
range between 0 and 1, predictions can be made based upon a default value. 
For example, the default prediction, `Yes`, could be made for any individual 
for whom the probability of pain reliever misuse and abuse is greater than
chance, $p(PRLMISAB) > 0.05$. Logistic regression uses a maximum likelihood 
method to predict the coefficient estimates that correspond as closely as 
possible to the default state. In other words, the model will predict a 
number close to one for individuals who have misused or abused pain
relievers and a number close to zero for individuals who have not 
\cite{james13}. The distribution of probabilities in the logit model has 
an S-shaped curve. The coefficients estimates obtained from a logistic 
regression model, ($beta_0$, $beta_1$ ... $beta_k$) are selected in order 
to maximize the likelihood function, and are interpreted as an indication 
of the log-odds change in the outcome variable that is associated with a 
one-unit increase in a predictor variable ($X_i$...$X_j$). 

Linear classification models differ in terms of (1) how they measure how 
well a particular combination of coefficients and intercept fit the training 
data, and (2) the type of regularization that is applied (e.g, L1, L2).
Linear Discriminant Analysis (QDA) is another commonly used model



]

Quadratic Discriminant Analysis (QDA)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Non-linear Classifier Models}

Support Vector Machines


Naive Bayes


Neural Networks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decision Tree Models}

Decision tree models are widely used for classification and regression. Tree 
models ``learn'' a hierarchy of if-else questions that are represented in the
form of a decision tree. Building decision trees proceeds from a root node as 
the starting point and continues through a series of decisions or choices.
Each node in the tree either represents either a question or a terminal node 
(i.e.,leaf) that contains the outcome. Applied to a binary classification task, 
the decision tree algorithm \emph{learns} the sequence of if-else questions 
that arrives at the outcome most quickly. For continuous features, the 
decisions are expressed in the form of, ``Is feature x larger than value y?''

In constructing the tree the algorithm searches through all
possible decisions or tests and finds a solution that is most informative 
about the target outcome \cite{muller17}. A decision tree classifier is used 
for binary or categorical targets, and decision tree regression is used for 
continuous target outcomes. The recursive branching process of tree based 
models yields a binary tree of decisions, with each node representing a test 
that considers a single feature. This process of recursive partitioning is 
repeated until each leaf in the decision tree contains only a single target. 
Prediction for a new data point proceeds by checking which region of the 
partition the point falls in, and predicting the majority in that feature space. 
The main advantage of tree based models is that they require little adjustment 
and are easy to interpret. A drawback is that they can lead to complex models 
that are highly overfit to the training data. A common strategy to prevent 
overfitting is \emph{pre-pruning}, which stops tree construction early by 
limiting the maximum depth of the tree, or the maximum number of leaves. 
One can also set the minimum number of points in a node required for splitting. 
Another approach is to build the tree and then remove or collapse nodes with 
little information, which is called \emph{post-pruning}. Decision trees work 
well with features measured on very different scales, or with data that has 
a mix of binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Random Forests Classifier}

A random forest is a collection of decision trees that are slightly different 
from the others, which each overfits the data in different ways. The idea 
behind random forests is that overfitting can be reduced by building many 
trees and averaging their results. This approach retains the predictive power 
of trees while reducing overfitting. Randomness is introduced into the tree 
building process in two ways: (a) selecting a bootstrap sample of the data, 
and (b) selecting features in each node branch \cite{muller17,raschka17}. In 
building the random forest, we first decide how many trees to build (e.g., 10 
or 100), and the algorithm makes different random choices so that each tree is 
distinct. The bootstrapping method repeatedly draws random samples of size n 
from the dataset (with replacement). The decision trees are build on these 
random samples that are the same size as the original data, with some points 
missing and some data points repeated. The algorithm also selects a random 
subset of p features, repeated separately each node in the tree, so that 
each decision at the node branch is made using a different subset of features.
These two processes help ensure that all of the decision trees in the random
forest are different. 

The important parameters for the random forests 
algorithm are the number of sampled data points and the maximum number of 
features; the algorithm could look at all of the features in the dataset
or a limited number. A high value for \emph{maximum-features} will produce 
trees in the random forest that are very similar and will fit the data 
easily based on the most distinctive features, whereas a low value will 
produce trees that are very different from each other, and reduces over-
fitting. Random forests is of the most widely used ML algorithms that works 
well without very much parameter tuning or scaling of data. A limitation of 
this approach is that Random forests do not perform well with very high-
dimensional, data that is sparse such as text data.

\subsubsection{Gradient Boosted Tree Classifier}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Study Goals}

The main hypothesis is that pain reliever misuse and abuse (PRLMISAB) can be
predicted by demographic features, medication use, and use of illicit drugs. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Summary of Variables in the NSDUH 2015-16 Aggregated Data Set}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    \textit{Dependent Variable} & Label \\
    \midrule
    Prescription Opioid Pain Reliever Misuse and Abuse (0-12 Scale)& PRLMISAB  \\
    \midrule
    \textit{Demographic Variables}&   \\
    \midrule
    Age Category (1=12-17 years, 2=18-25, 3=26-34, 4=35-49, 5=50 and older)& AGECAT \\
    Biological Sex (0=Male, 1=Female)& SEX  \\
    Marital Status (0=Unmarried, 1=Divorced, 2=Widowed, 3=Married)& MARRIED  \\
    Education (1=H.S. or Less, 2=H.S. Grad., 3=Some College, Â 4=College Grad.)& EDUCAT  \\
    Size of City/Metropolitan Region (1=Rural, 2=Small, 3=Large)& CTYMETRO  \\
    Health Problems Aggregated  (0-10 scale)& HEALTH  \\
    Mental Health, Aggregated: adult depression, emotional distress (0-10 scale)& MENTHLTH  \\
    Treatment for Drugs and Alcohol in past year, Aggregated (0-5 scale)& TRTMENT  \\
    Mental Health Treatment, Aggregated (Likert scale, 1-10)& MHTRTMT  \\
    \midrule
    \textit{Medication and Drug Use Variables}& \\
    \midrule
    Tranquilizer use, past year, Aggregated (Likert scale, 0-5)& TRQLZRS \\
    Sedative use, past year, Aggregated (0-5 scale)& SEDATVS  \\
    Heroin use, past year, Aggregated (0-5 scale)& HEROINUSE  \\
    Cocaine and Crack Cocaine Use in past year, Aggregated  (0-5 scale)& COCAINE  \\
    Amphetamine and Methamphetamine Use in past year, Aggregated (0-5 scale)& AMPHETMN  \\
    \bottomrule
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}

\subsection{The Data}

The NSDUH public data files for 2015 and 2016 were downloaded from the 
Substance Abuse and Mental Health Data Archive (SAMHDA) website, and the 
data sets were extracted, and saved as data frame objects in a python 
interactive notebook \cite{mckinney17, vanderplas17}. The 2015 NSDUH data set 
consisted of N=57146 respondents with 2667 variables and the 2016 data set had 
N=57897 individuals and 2665 variables, resulting in a total sample of N=114,043 
observations (53873 male, 60165 female). As described in the NSHUD codebook, 
the sampling design is weighted across states by population size, drawing more 
heavily from eight states with the largest populations (CA, FL, IL, MI, NY, OH, 
PA, TX), for a representative distribution that accounts for approximately 
48 percent of the U.S. population. For 2015, the weighted survey screening 
response rate  was 81.94\% and the weighted interview response rate was 71.2\% 
\cite{samhsa18}. Identifying information in the survey is collapsed (e.g., age 
categories); variables related to ethnicity, immigration status, and state 
identifiers are removed to ensure confidentiality. The data frames were subset 
by column to select approximately 90 variables that included common demographic 
characteristics, physical health, mental health, medication usage, and use of 
illicit drugs. The following steps were taken to detect and remove any 
inconsistencies in the data: (1) Missing values (i.e., NaN) were removed. 
(2) Blanks, non-responses, or legitimate skips (e.g., 99, 991, 993) were 
recoded to zero. (3) Dichotomous responses (e.g., Yes=1 / No=2) were recoded 
so that No=0. (4) Categorical variables were recided to be consistent with amount 
or degree  (e.g., 1=low, 2=med, 3=high). (5) Outliers were identified and excluded 
from the data set (n=5).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Aggregated Variables}

Related variables were combined to create aggregated features. For example, 
A single variable `HEALTH` was created by combining scores for overall health 
(reverse scored), previous diagnosis of STDs, hepatitis, HIV, cancer, and 
any previous hospitalization. The variable for mental health (MENTHLTH) 
aggregated responses for adult depression, emotional distress, suicidal 
thoughts or plans. Any prescription opioid pain medication use (in past year) 
was assessed by combining binary responses for ten of the most commonly used 
prescription pain medications (e.g., Hydrocodone, Oxycodone, Tramadol, 
Morphine, Fentanyl, Oxymorphone, Demerol, Hydromorphone). The majority of 
questions related to substance use had dichotomous responses that were summed 
to create single measures for: Tranquilizers, Sedatives, Heroin, Cocaine, 
and Amphetamines. Because hallucinogens varied greatly in strength and type 
(e.g., marijuana, psilocybin, MDMA, LSD), they were not considered in the 
analysis. The variables for drug treatment and mental health treatment 
combined responses for any inpatient care, outpatient care, treatment at a 
clinic, emergency room visits, or hospital stays. A measure of any previous 
pain reliever misuse or abuse combined responses for the misuse, abuse, or 
dependency on prescription pain relievers in the past year, month. The target 
variable was a dichotomous measure of any pain reliever misuse or abuse at any 
point. The subset data frame consisted of 20 features and 114038 observations 
and was exported to CSV file. Four variables were excluded from analysis 
(PRLANY, PRLMISAB, HEROINEVR, HALUCNG). Table 2 shows the complete list of 
variables used for predictive modeling. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Classification Models for Predict Pain Reliever Misuse 
  and Abuse and Main Parameters}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    Model & Main Parameter \\
    \midrule
    K-Nearest Neighbors & Number of Neighbors = 4 \\
    Logistic Regression & Number of Features = 15 \\
    Linear Discriminant Analysis (LDA) & Number of Features = 15 \\
    Support Vector Machines (SVM) & Kernel = linear \\
    Naive Bayes & Cost C = 0.01 \\
    Neural Network & Hidden Layers = 2 \\
    Decision Trees & Tree-Depth = 4 \\ 
    Random Forests & Number of Trees = 100 \\
    Boosted Trees & Learning Rate = 0.01 \\ 
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Exploratory Data Analysis}

6,343 individuals reported misusing pain medication at some point
(3221 males, 3122 females), but only 956 respondents had used heroin (570 males, 
386 females). 

Of  26,736 were male and 30,410 
female; 6,343 individuals reported misusing pain medication at some point
(3221 males, 3122 females), but only 956 respondents had used heroin (570 males, 
386 females). 

Only few people 
self-described as high in depression reported low Prescription Opioid PRL 
misuse and abuse. 

 The target variable was Heroin Use (HEROINEVR). Next, 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparison of Classifier Models}

The dataset was divided into the training set and test set using a 75-25 
percent split. Each model was fit to the training set, new values were 
predicted on the holdout scores in the testing set. The main parameter settings 
for each model are reported in Table 4. Model performance was evaluated 
and reported in the confusion matrix (Table 3), which include metrics of
Accuracy (percentage), Sensitivity (i.e., Recall), Precision, and $f_1-score$. 

 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\subsubsection{Logistic Regression}
 
The logistic regression classifier 
was fit to the training data in Scikit-Learn, and the model was validated on 
the test data. By default, the model applies L2 penalty (Ridge). The training 
set accuracy was 0.983 and the test set accuracy was 0.984. The parameter `C` 
determines the strength of regularization, with higher values of C providing
greater regularization. The L1 penalty (Lasso) limits the values of most 
coefficients to zero, creating a more interpretable model that uses only a 
few features. Figure 4 plots the coefficients of logistic regression classifier 
for heroin use with the L1 Penalty (Lasso) under different values of parameter 
C. The default setting, C=1.0, provides good performance for train and test 
sets, but the model is very likely underfitting the test data. Using a higher
value of C fits a more flexible model and generally gives improved accuracy 
for both training and tests sets. Using a value of C=100 yielded training set
accuracy of 0.98 and test set accuracy of 0.98. Figure 4 shows that the 
features coefficient values did not change much according to the values of
parameter C, and the accuracy values were approximately the same for all 
values of C. Examination of the coefficients from the logistic regression 
classifier revealed the three features which were most closely associated 
with Heroin use were: Prescription Opioid Pain Reliever (PRL) Misuse ever 
(as predicted), Cocaine Use, and Amphetamine use, respectively.

The main parameter for linear classification models is the
regularization parameter C. High values of C correspond to less regularization 
and the model will fit the training set as best as possible, stressing the 
importance of each individual data point to be classified correctly. By 
contrast, with low values of C, the model puts more emphasis on finding 
coefficient vectors (i.e., weights) that are close to zero, trying to adjust to 
the majority of data points. In addition, the penalty parameter influences the 
coefficient values of the linear model. The L2 penalty (Ridge) uses all 
available features, but pushes the coefficient values toward zero. The L1 
penalty (Lasso) sets the coefficient values for most features to zero, and uses 
only a subset of features for improved interpretability. This analysis used a 
logistic regression classifier to predict  from demographic 
attributes, mental health, prescription opioids, medication use, misuse, 
and illicit drug use. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Confusion Matrices and Performance Metrics for Predictive Models of 
  Pain Reliever Misuse and Abuse}
  \label{tab:freq}
  \begin{tabular}{llllllll}
    \toprule
    Model& & Confusion Matrix & & Accuracy & Sensitivity & Precision & F1-Score \\
    \midrule
    K-Nearest Neighbors & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25114 & 320 & 89.5\% & 0.900 & 0.870 & 0.870 \\
     & PRL Misuse & 2609 & 467 &  &  &  & \\
    \midrule
    Logistic Regression & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25002 & 2509 & 90\% & 0.986 & 0.909 & 0.946 \\
     & PRL Misuse & 344 & 654 &  &  &  & \\
    \midrule
    Linear Discriminant Analysis (LDA) & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 24668 & 2239 & 89.8\% & 0.973 & 0.917 & 0.944 \\
     & PRL Misuse & 678 & 924 &  &  &  & \\
    \midrule
    Quadratic Discriminant Analysis (QDA) & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 23165 & 1780 & 86.1\% & 0.914 & 0.9929 & 0.921 \\
     & PRL Misuse & 2181 & 1383 &  &  &  & \\
    \midrule
    Support Vector Classifier (SVC) & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25201 & 233 & 90.4\% & 0.900 & 0.890 & 0.880 \\
     & PRL Misuse & 2514 & 562 &  &  &  & \\
    \midrule
    Naive Bayes Classifier & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25345 & 3133 & 89\% & 0.999 & 0.890 & 0.941 \\
     & PRL Misuse & 1 & 30 &  &  &  & \\
    \midrule
    Neural Net  (MLP) & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25088 & 346 & 90.6\% & 0.910 & 0.890 & 0.880 \\
     & PRL Misuse & 2366 & 710 &  &  &  & \\
    \midrule
    Decision Trees & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25042 & 2572 & 89.9\% & 0.988 & 0.907 & 0.946 \\
     & PRL Misuse & 304 & 591 &  &  &  & \\
    \midrule
    Random Forests & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25054 & 2518 & 90.1\% & 0.989 & 0.909 & 0.947 \\
     & PRL Misuse & 292 & 645 &  &  &  & \\
    \midrule
    Gradient Boosted Trees & & No Misuse & PRL Misuse &  &  &  & \\
     & No Misuse & 25415 & 19 & 89.6\% & 0.900 & 0.890 & 0.850 \\
     & PRL Misuse & 2954 & 122 &  &  &  & \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure2.pdf}
  \caption{Neural Net Classifier: Multilayer Perceptron with Single Hidden Layer}
  \label{f:Figure2}
\end{figure}












\subsubsection{K-Nearest Neighbors (KNN)}


one of the simplest classification algorithms, the K-Nearest Neighbors (KNN) 
model takes a set of data points and classifies a new data point based on the
(by default) Euclidean distance to its nearest neighbors. the KNN model was 
fit on the training set, and  New values were predicted on the test set. In 



The advantage of the KNN classifier is that it provides a solution that is easy 
to understand. A limitation of KNN is that it does not perform well with a large 
number of features (100 or more) or with sparse datasets with many values equal to zero. ,  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Decision Tree Classifier}

The following analysis used the \emph{Decision Tree Classifier} package in 
Scikit-Learn, which only does pre-pruning. First, the decision model was build
using the default setting of a fully developed tree until all leaves are pure. 
The random state` features is fixed to break ties internally. Accuracy on the
training set was 0.99 and test set accuracy was 0.974. Without restricting 
their depth, decision trees can become complex; unpruned trees are prone to 
overfitting and do not generalize well to new data. Limiting the depth of 
tree decreases overfitting, which results in lower training set accuracy, 
but improved performance on the test set. 

Next, pre-pruning was applied, with 
a maximum depth of 4, which means the algorithm split on four consecutive
questions. Training set accuracy of the pruned tree was 0.985 and test set
accuracy was 0.984. Even with a depth of 4, the tree can become a bit complex.
Figure 5 shows a partial view of the decision tree classifier of heroin use 
(the entire tree was too wide to include as a legible Figure), and the full 
tree image is available in the notebook BDA-Analytics-Classifier-Heroin.ipynb 
\cite{classifyH}. 

The decision tree shows the top features that the algorithm 
split on to classify heroin use. One way to interpret a decision tree it by 
following the sample numbers represented at the test split for each node. 
The classifier algorithm selected Cocaine Use (aggregated score) as the root 
node of the decision tree. The branch to the left  side of the tree represents 
samples with a score equal to or less than 1.5 (n=40956), whereas the branch 
to the right represents samples with a Cocaine Use score greater than 1.5
(n=1903). The second split on the right occurs for Any Prescription Opioid 
Pain Reliever Use (PRLANY), with n=1443 having a score less than or equal 
to 3.5, and n=460 respondents with a PRL score greater than 3.5. In other 
words, of those respondents who reported relatively high Cocaine use, a small
portion also reported relatively high Prescription Opioid PRL use. Instead of 
looking at the whole tree, features importance is a common summary function 
that rates how important each feature is for the classification decisions 
made in the algorithm. Each feature is assigned an importance value between 
0 and 1; with a value of 1 indicating the feature perfectly predicts the 
target and a value of 0 meaning that the feature was not used at all. 
Feature importance values also always sum to 1. A feature may have a low 
feature importance value because another feature encodes the same information. 
The top two important features for classifying Heroin Use were Cocaine Use 
and Any Prescription Opioid PRL Use, with smaller importance given to Opioid 
PRL Misuse Ever and Prescription Opioid PRL Misuse and Abuse. 

The Decision Tree Classifier package in Scikit-Learn was used to build the 
tree model, pre-pruning was applied with a maximum depth of 4, which means 
the algorithm split on four consecutive questions. The training set accuracy 
of the pruned tree was 0.902 and test set accuracy was 0.902. Figure 9 shows 
a partial view of the decision tree classifier of prescription opioid misuse
(the full tree is included in the BDA-Analytics-Classifier-PRL.ipynb 
notebook) \cite{classifyPRL}. 

As Figure 9 shows, the decision tree classifier
selected Cocaine Use as the root note, that branched by the test score equal
to or less than 0.5 (any Cocaine Use). At the second node, on the branch to 
the right n=5015 samples were further divided according to heroin use, with 
n=1913 having a score greater than 0.5 (any Heroin Use). At the third node
on the right branch, samples were selected according to Tranquilizer
medication use, with n=1419 scoring positively. On the left branch, the 
second node selected was Drug Treatment, with n=2844 respondents scoring
positively that they had received Drug Treatment. Feature importance of
the decision tree classifier selected Cocaine Use as the most informative
feature for Prescription Opioid PRL Misuse. Following afterwards, 
Tranquilizer Use, Drug Treatment, and Heroin Use were tied for second place. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Random Forests Classifier}

Random forests is an ensemble approach that builds many trees and averages 
their results to reduce overfitting. The model was build using the 
\emph{Random Forest Classifier} package in Scikit-Learn. The parameters of 
interest for building random forests are: (a) the number of trees 
(n-estimators), (b) the number of data points for bootstrap sampling 
(n-samples), and (c) the maximum number of features considered at each node 
(max-features). The max-features parameter determines how random each tree is, 
with smaller values of max-features resulting in trees in the random forest 
that are very different from each other. This analysis applied a random forest 
consisting of 100 trees to classify Heroin Use, and the random state was set to 
zero. The training set accuracy was 0.999 and the test set accuracy was 0.984. 
Often the default settings for random forests work well, but we can apply
pre-pruning as with a single tree, or adjust the maximum number of features. 
Feature importance for random forests is computed by aggregating the feature 
importance over trees in the random forest, and random forests gives
non-zero importance to more features than a single tree. Typically random
forests provide a more reliable measure of feature importance than the
feature importance for a single tree. Figure 6 shows the feature importance 
of the random forests classifier for heroin use with 100 trees. Similar to
the single tree, the random forest selected Cocaine Use as the most
informative feature in the model, followed by Any PRL Use, which is an 
aggregated measure of prescription opioid medication use. Following after 
that, several features were tied for third place of importance, namely 
Education Level, Overall Health, Age Category, and Pain Reliever Misuse 
and Abuse. Random forests provides much of the same benefit as decision
trees, while compensating for some of their shortcomings of overfitting.
Single trees are still useful for visually representing the decision process.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
  \caption{Decision Tree Classification of Heroin Use (Partial View)}
  \label{f:Figure5}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Random Forests Classifier}

The Random Forest Classifier package in Scikit-Learn was used to classify
Prescription Opioid PRL Misuse as the target variable, with 100 trees. The 
model accuracy for the training set was 0.955 and the test set accuracy was 
0.896, which suggests that the model overfit the data. Figure 10 shows the 
feature importance of the random forests classifier for Prescription Opioid 
PRL Misuse. As Figure 10 shows, several features were identified as important
for classifying Prescription Opioid PRL Misuse. The random forest selected 
Overall Health as the most informative feature in the model, followed by 
Cocaine Use, Education Level, Age Category, and Size of City Metropolitan 
region. Because of the additional features included as important, gradient 
boosting was performed to clarify the feature importance.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Gradient Boosting Classifier Tree}

Gradient boosting machines is another ensemble method that combines multiple
decision trees for regression or classification by building trees in a serial 
fashion, where each tree tries to correct for mistakes of the previous one
\cite{muller17}. Gradient boosted regression trees use strong pre-pruning, 
with shallow trees of a depth of one to five. Each tree only provides a good
estimate of part of the data, but combining many shallow trees (i.e., ``weak 
learners''), the use many simple models iteratively improves performance. In 
addition to pre-pruning and the number of trees, an important parameter for 
gradient boosting is the learning rate, which determines how strongly each
tree tries to correct for mistakes of previous trees. A high learning rate
produces stronger corrections, allowing for more complex models. Adding
more trees to the ensemble also increases model complexity. Gradient boosting
and random forests perform well on similar tasks and data; it is common to
first try random forests and then include gradient boosting to attain 
improvements in accuracy of the learning model. This analysis used the 
\emph{Gradient Boosting Classifier} from Scikit-Learn to classify Heroin Use, 
with the default setting of 100 trees of maximum depth of 3, and a learning 
rate of 0.1. The model was build on the training set and evaluated on the test 
set, with both training set and test set accuracy equal to 0.984. To reduce
overfitting, pre-pruning could be implemented by reducing the maximum depth, 
or by reducing the learning rate. Figure 7 shows that the feature importance 
for the gradient boosting classifier tree looks similar to the feature 
importance for random forests, but the gradient boosting has decreased the 
importance of many features to zero. Again Cocaine is selected as the most 
imformative features, followed by Any Opioid PRL Use. In addition to 
Prescription Opioid PRL Misuse and Abuse, the gradient boosting classifier 
selected Amphetamine Use as an informative feature of Heroin Use. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure7.pdf}
  \caption{Feature Importance for Gradient Boosting Classifier for Heroin Use}
  \label{f:Figure7}
\end{figure}

The Gradient Boosting Classifier from Scikit-Learn was used to classify 
Prescription Opioid PRL Misuse, using the default setting of 100 trees, of 
maximum depth of 3, and a learning rate of 0.1. The model accuracy for the
training set was 0.894 and accuracy for the test set was 0.893. Gradient 
boosting typically improves test set accuracy by using many simple models 
iteratively. In this case, model accuracy for gradient boosting was no better 
than random forests, and this is because the default parameter settings were
used; further parameter tuning is needed to improve model performance. Feature 
importance was a primary interest for identifying features related to '
prescription opioid abuse. Figure 11 shows the feature importance for the 
gradient boosting classifier tree. As Figure 11 shows, several features were 
important for classifying prescription opioid misuse, and contrary to the 
random forests, gradient boosting selected Tranquilizer use as the most 
informative feature. Following closely in importance were Heroin Use and Age 
Category. Tied for fourth place were Cocaine Use and Treatment, with Mental 
Health (depression) coming in fourth in terms of feature importance. This 
result illustrates that several features are important for understanding 
Prescription Opioid Misuse, and the relations among features may be complex.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISCUSSION}

The results show that rates of prescription opioid use, misuse, and abuse are
much higher than use of illicit opioids such as heroin and fentanyl. The use 
of Hydrocodone (Vicodan) was double the rate of Oxycodone use (Oxycodone) 
across almost all age groups. The use of traditional prescription opioids 
was greater than reported use of synthetic opioids. Illicit drug use was 
highest for respondents between the ages of 18 to 25. In terms of mental 
health, more individuals between 18 to 25 years reported experiencing a major 
depressive episode (in adulthood) than any other age group. In terms of the 
so-called \emph{treatment gap}, almost twice as many respondents between 
18 to 25 years who felt a need for substance use treatment, had not received
treatment, than younger individuals between 12 to 17 years. The large majority 
of respondents (approximately 90 percent) had not misused prescription opioid 
pain relievers or used heroin. However, of those individuals who reported 
misusing prescription opioid pain relievers, almost twice as many had also
used heroin than had not (see Figure 1), which partially supports the 
hypothesis that prescription opioid use is associated will use of illicit 
opioids such as heroin. Prescription opioid misuse and heroin use was also
higher in large metropolitan areas than smaller cities or rural areas, but
a small portion of individuals in non-metropolitan regions reported very
high levels of prescription opoioid misuse. These data points may represent 
outliers, but a large sample would allow for analysis of how opioid misuse 
and addiction differ for smaller rural regions versus large urban areas. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparison of Classifier Models}

Several classifier algorithms were used to identify relevant features for 
predicting heroin use and prescription opioid misuse. Comparing the performance 
of different algorithms is helpful for  selecting the best model. Test set
accuracy was comparable across models for both Heroin Use (0.98) and 
Prescription Opioid PRL Misuse (0.89-0.90). Logistic Regression provided the
feature coefficients for different values of the regularization parameter C. 
The Decision Tree classifier provided a easy to use, interpretable visual of
the decisions involved at each step of classification. Random forests provides
a more reliable indication of features importance than a single tree, 
whereas the gradient boosting classifier included additional tuning 
parameter for a more powerful model and more interpretable analysis of
feature importance. Each classifier method provides a different level of
analysis. For classifying heroin use, the logistic regression classified
showed that Prescription Opioid PRL Misuse had the highest coefficient value, 
but the tree-based classifiers each identified Cocaine Use as the most
informative feature for predicting heroin use. For classifying Prescription 
Opioid PRL Misuse, logistic regression showed that Treatment had the highest
coefficient value, but the tree based models each differed in selecting the
most important features. Decision trees indicated that Cocaine Use was most
informative, the random forests classifier selected health as the most
important feature, and the gradient boosting model selected Tranquilizer use
as most informative of prescription opioid PRL misuse. The different model 
each have their advantages and limitations, logistic regression provides the
coefficients, but random forests and gradient boosting are helpful for 
identified sets of important features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Limitations}

Surveys data may be biased to some degree, but measures of confidentiality and 
anonymity help to assure more accurate disclosures. 

The main goal of this project was to identify features relevant for predicting 
opioid addiction by classifying cases according to heroin use. Only a small 
proportion of the sample reported having used heroin, and scores for mental
health issues were very low. A limitation of survey data is that responses may 
be biased by under-reporting or minimizing the use of illicit or illegal 
substances. People may also be reluctant to disclose mental health issues or 
health problems (e.g., STDs, HIV status, suicide attempts). It is possible
that this sample is representative of the frequency of opioid use and misuse
in the larger population. Recent statistics from the CDC show that heroin use
has increased among most demographics groups, with an average estimated rate 
of approximately 2.6 percent between 2011-2013 \cite{cdc16}. The rate of heroin 
use reported in the NSDUH-2015 sample was 1.6 percent. Therefore, it seems
that the actual rate of heroin use in the U.S. population may not be accurately
reflected in this sample. Another limitation is that the project dataset was 
constructed as a subset of features from the NSDUH-2015 data. Ninety 
attributes were selected out of 2666 features in the original dataset, and many 
features were combined to create aggregated variables for health, mental 
health, prescription opioid misuse and abuse, drug treatment, mental health
treatment. Future research could include a more comprehensive selection of
features to identify the set of features relevant for predicting opioid
dependency and addiction. An important challenge for making sense of big data 
is developing analytic tools adequate to handle large volumes of data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extension to Big Data}

A general tenet of big data is that, ``More data is always better.'' The 
methods used in this project could be extended to better approximate big data 
for predicting opioid use in the following ways: (1) Include a larger 
selection of features from the attributes in the NSDUH-2015 dataset; (2) 
Include survey data from previous years (e.g., 2005-2015) for a larger sample;  
and (3) Obtain a broader sample from the population of patients who are 
taking prescribed opioid medications. The most immediate step would be to 
include additional features for use with the classifier models. Additional 
data from the NSDUH was downloaded from previous years (2012 to 2014); 
preliminary examination of the data revealed inconsistencies in questions 
and prescription opioid medications that would need to be resolved in order 
to combine data from multiple years. Data cleaning can be a time consuming 
process, but important for obtaining usable data. Unfortunately, owing to 
constraints of time for completing the project, it was not possible to
integrate data from previous years into the project dataset. In working with
big data, there are there are several steps involved in the consolidation of 
data from multiple sources into a single dataset (in addition to data 
cleaning), which include extraction, integration, and aggregation of features  
\cite{rahm00}. A future study could integrate data from different years, 
using a broader set of features, with more inclusive sample representative
of the larger population, and integrate data from multiple sources. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opioid Addiction and Epidemic Spreading}

Drug addiction has many similar characteristics to other chronic medical 
illnesses, but there are unique challenges to the treatment of addiction
\cite{marsch12, swendson16}. In drug rehabilitation treatment programs, 
patients undergo intense detoxification that reduces their drug tolerance, 
but are then released back into the environments associated with their drug 
use, putting them at high risk for relapse and potential drug overdose 
\cite{johnson11}. If the prescription opioid crisis is a genuine epidemic, 
we must consider the process of spreading or diffusion of contagion. Epidemic 
spreading is a dynamic process based on networks of direct person-to-person 
contact and indirect exposure via transportation pathways \cite{Colizza06}. 
Epidemics are quantified in terms of the proportion of the population infected, 
those yet to be infected, and the rate of transmission. Potentially everyone
is at risk of becoming dependent or addicted to prescription medications or 
illicit opioids. In terms of the opioid epidemic, rather than labeling persons 
as infected or uninfected, it is more useful to consider people as either 
susceptible to dependence and addiction or less susceptible. Furthermore, 
the structure of the contact network can influence epidemic spreading
\cite{pastor01}. For example, in the case of simple contagion, weak 
ties among acquaintances or infrequent associations provide shortcuts between 
distant nodes that reduce distance within the network \cite{granovetter73} 
which can facilitate the spread of contagion, or in this case drug use. 
Furthermore, contact networks for drug use may have ``small world'' properties
where a small number of nodes have a high number of connection that can 
rapidly transmit contagion throughout the network \cite{watts98}. Network 
analysis may help to identify the underlying structure of the contact network
of opioid use, to examine pathways and points of contact in the misuse and 
abuse of prescription opioid medications. According to a classical conditioning
model of addiction, situational cues or events can elicit a motivational state 
underlying relapse to drug use. Addictive behavior can be also be reinstated 
after extinction of dependency by exposure to drug-related cues or stressors 
in the environment \cite{shaham03}. Future research could use social network 
modeling to explore how drug dependency and addiction are subserved by patterns 
of social interaction. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

This project compared several classification algorithms to predict heroin use 
and prescription opioid misuse and abuse. The results provided partial support
for the hypothesis that prescription opioid misuse is associated with the use
of illicit opioids such as heroin. Several features were identified as 
important for classifying heroin use, including Cocaine Use, Amphetamine Use, 
and any prescription opioid medication use. In regards to predicting heroin
use, it appears the use of other illicit drugs such as Cocaine and Amphetamine 
was perhaps more informative than any prescription opioid use or misuse. Heroin 
use was selected as important for classifying prescription opioid pain reliever 
misuse, but additional factors also played as role, including tranquilizer use,
age category, overall health, cocaine use. Substance treatment had the largest
regression coefficient, suggesting that people who are misusing prescription
opioid pain medication are also more likely to be in drug treatment programs. 
The direction of these effects cannot be determined owing to the nature of the 
analyses. On the one hand individual misusing or abusing prescription opioids 
may also be using heroin. Alternatively, individuals with a susceptability for 
opioid use may be equally likely to have use heroin and also to have misused 
prescription opioids. A general conclusion is that of those individuals who 
reported misusing prescription opioid medications, twice as said they had used
heroin than reported they had not used heroin. The results do not provide 
sufficient evidence to rule out alternative hypotheses. Given the relatively 
low rates of opioid and heroin in this sample, additional evidence is needed to 
resolve this question. The study can provide information to raise awareness 
about the risk factors for prescription opioid addiction and may help reduce 
opioid overdose deaths. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acks}

Portions of this paper were completed as part of a course project in Big Data 
Applications and Analytics taugt by Dr. Gregor von Laszewskin at Indiana 
University in the Fall 2017. Thanks to the Professor von Laszewski and the
Teaching Assistants, Juliette Zurick, Miao Jiang, Hungri Lee, Grace Li, and 
Saber Sheybani Moghadam for helpful comments and feedback.

\end{acks}

\bibliographystyle{unsrt} %%ACM-Reference-Format%%
\bibliography{report} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Code References}
All code, notebooks, files, and folders for this project can be found in the

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\input{issues}

\end{document}
